{"wandb": "True", "wandb_logger": "<deepchem.models.wandblogger.WandbLogger object at 0x7f27530df280>", "batch_size": "100", "learning_rate": "0.001", "nb_epoch": 30, "init_params": "{'wandb': True, 'wandb_logger': <deepchem.models.wandblogger.WandbLogger object at 0x7f27530df280>, 'batch_size': 100, 'learning_rate': 0.001}", "mode": "regression", "num_classes": "None", "model_dir": "/tmp/tmpl8kngyre", "custom_logger": "None", "model": "AttentiveFP(\n  (model): AttentiveFPPredictor(\n    (gnn): AttentiveFPGNN(\n      (init_context): GetContext(\n        (project_node): Sequential(\n          (0): Linear(in_features=30, out_features=200, bias=True)\n          (1): LeakyReLU(negative_slope=0.01)\n        )\n        (project_edge1): Sequential(\n          (0): Linear(in_features=41, out_features=200, bias=True)\n          (1): LeakyReLU(negative_slope=0.01)\n        )\n        (project_edge2): Sequential(\n          (0): Dropout(p=0.0, inplace=False)\n          (1): Linear(in_features=400, out_features=1, bias=True)\n          (2): LeakyReLU(negative_slope=0.01)\n        )\n        (attentive_gru): AttentiveGRU1(\n          (edge_transform): Sequential(\n            (0): Dropout(p=0.0, inplace=False)\n            (1): Linear(in_features=200, out_features=200, bias=True)\n          )\n          (gru): GRUCell(200, 200)\n        )\n      )\n      (gnn_layers): ModuleList(\n        (0): GNNLayer(\n          (project_edge): Sequential(\n            (0): Dropout(p=0.0, inplace=False)\n            (1): Linear(in_features=400, out_features=1, bias=True)\n            (2): LeakyReLU(negative_slope=0.01)\n          )\n          (attentive_gru): AttentiveGRU2(\n            (project_node): Sequential(\n              (0): Dropout(p=0.0, inplace=False)\n              (1): Linear(in_features=200, out_features=200, bias=True)\n            )\n            (gru): GRUCell(200, 200)\n          )\n        )\n      )\n    )\n    (readout): AttentiveFPReadout(\n      (readouts): ModuleList(\n        (0): GlobalPool(\n          (compute_logits): Sequential(\n            (0): Linear(in_features=400, out_features=1, bias=True)\n            (1): LeakyReLU(negative_slope=0.01)\n          )\n          (project_nodes): Sequential(\n            (0): Dropout(p=0.0, inplace=False)\n            (1): Linear(in_features=200, out_features=200, bias=True)\n          )\n          (gru): GRUCell(200, 200)\n        )\n        (1): GlobalPool(\n          (compute_logits): Sequential(\n            (0): Linear(in_features=400, out_features=1, bias=True)\n            (1): LeakyReLU(negative_slope=0.01)\n          )\n          (project_nodes): Sequential(\n            (0): Dropout(p=0.0, inplace=False)\n            (1): Linear(in_features=200, out_features=200, bias=True)\n          )\n          (gru): GRUCell(200, 200)\n        )\n      )\n    )\n    (predict): Sequential(\n      (0): Dropout(p=0.0, inplace=False)\n      (1): Linear(in_features=200, out_features=1, bias=True)\n    )\n  )\n)", "fit_params": "{'nb_epoch': 30}", "model_dir_is_temp": "True", "model_class": "<class 'deepchem.models.torch_models.attentivefp.AttentiveFP'>", "loss": "<deepchem.models.losses.L2Loss object at 0x7f2750badd30>", "output_types": "['prediction']", "_loss_fn": "<deepchem.models.torch_models.torch_model._StandardLoss object at 0x7f2750badd60>", "optimizer": "<deepchem.models.optimizers.Adam object at 0x7f2750baddf0>", "tensorboard": "False", "regularization_loss": "None", "device": "cuda", "log_frequency": "100", "_prediction_outputs": "[0]", "_loss_outputs": "[0]", "_variance_outputs": "[]", "_other_outputs": "[]", "_built": "True", "_output_functions": "{}", "_optimizer_for_vars": "{}", "_self_loop": "True", "_global_step": "1590", "_pytorch_optimizer": "Adam (\nParameter Group 0\n    amsgrad: False\n    betas: (0.9, 0.999)\n    eps: 1e-08\n    lr: 0.001\n    maximize: False\n    weight_decay: 0\n)", "_lr_schedule": "None"}